{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 1\n",
        "Βήμα 1: Φόρτωση δεδομένων (mfccs)\n"
      ],
      "metadata": {
        "id": "WyAzeZmN0iR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgq_ifjyX2jk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Load the training data\n",
        "X_train = np.load('/content/train/X.npy', allow_pickle=True)\n",
        "y_train = np.load('/content/train/labels.npy', allow_pickle=True)\n",
        "\n",
        "# Load the validation data\n",
        "X_val = np.load('/content/val/X.npy', allow_pickle=True)\n",
        "y_val = np.load('/content/val/labels.npy', allow_pickle=True)\n",
        "\n",
        "# Load the test data\n",
        "X_test = np.load('/content/test/X.npy', allow_pickle=True)\n",
        "y_test = np.load('/content/test/labels.npy', allow_pickle=True)\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val = X_val.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'classical': 0, 'blues': 1, 'hiphop': 2, 'rock_metal_hardrock': 3}\n",
        "y_train = np.array([label_mapping[label] for label in y_train], dtype=np.long)\n",
        "y_val = np.array([label_mapping[label] for label in y_val], dtype=np.long)\n",
        "y_test = np.array([label_mapping[label] for label in y_test], dtype=np.long)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "tensor_X_train = torch.Tensor(X_train)\n",
        "tensor_y_train = torch.Tensor(y_train)\n",
        "tensor_X_val = torch.Tensor(X_val)\n",
        "tensor_y_val = torch.Tensor(y_val)\n",
        "tensor_X_test = torch.Tensor(X_test)\n",
        "tensor_y_test = torch.Tensor(y_test)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(tensor_X_train, tensor_y_train)\n",
        "val_dataset = TensorDataset(tensor_X_val, tensor_y_val)\n",
        "test_dataset = TensorDataset(tensor_X_test, tensor_y_test)\n",
        "batch_size = 16\n",
        "shuffle = True\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 2: Ορισμός Νευρωνικού Δικτύου\n"
      ],
      "metadata": {
        "id": "vgnrSgYz2y4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(26, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 4)\n",
        "        self.fc4 = nn.Linear(4, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = FullyConnectedNetwork()\n"
      ],
      "metadata": {
        "id": "KYWeC1KT21FT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 3: Ορισμός διαδικασίας εκπαίδευσης\n"
      ],
      "metadata": {
        "id": "GTBkFKqo6sWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(num_epochs, optimizer, dataloader, loss_fn, model):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            loss = loss_fn(outputs, batch_y.long())\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "UT1izr0j6rq-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 4: Ορισμός διαδικασίας αξιολόγησης\n"
      ],
      "metadata": {
        "id": "J3tPeSQD7YhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(dataloader, loss_fn, model):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in dataloader:\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "\n",
        "            loss = loss_fn(outputs, batch_y.long())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            all_predictions.extend(predictions.tolist())\n",
        "            all_labels.extend(batch_y.tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    confusion_mat = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    return avg_loss, f1_macro, accuracy, confusion_mat\n"
      ],
      "metadata": {
        "id": "AHny0fiu7ZpV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 5: Εκπαίδευση δικτύου\n"
      ],
      "metadata": {
        "id": "xs4CzFtw8YIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n",
        "num_epochs = 30\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "trained_model = train_network(num_epochs, optimizer, train_dataloader, loss_fn, model)\n",
        "\n",
        "test_loss, test_f1_macro, test_accuracy, test_confusion_mat = evaluate_model(test_dataloader, loss_fn, trained_model)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test F1 Macro:\", test_f1_macro)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Confusion Matrix:\\n\", test_confusion_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HIUezB-8Zmi",
        "outputId": "ac4e06fb-75de-47b9-c2e4-f80cf328094f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 1.4354\n",
            "Epoch [2/30], Loss: 1.4255\n",
            "Epoch [3/30], Loss: 1.4177\n",
            "Epoch [4/30], Loss: 1.4115\n",
            "Epoch [5/30], Loss: 1.4066\n",
            "Epoch [6/30], Loss: 1.4026\n",
            "Epoch [7/30], Loss: 1.3995\n",
            "Epoch [8/30], Loss: 1.3970\n",
            "Epoch [9/30], Loss: 1.3949\n",
            "Epoch [10/30], Loss: 1.3933\n",
            "Epoch [11/30], Loss: 1.3920\n",
            "Epoch [12/30], Loss: 1.3909\n",
            "Epoch [13/30], Loss: 1.3901\n",
            "Epoch [14/30], Loss: 1.3894\n",
            "Epoch [15/30], Loss: 1.3888\n",
            "Epoch [16/30], Loss: 1.3884\n",
            "Epoch [17/30], Loss: 1.3880\n",
            "Epoch [18/30], Loss: 1.3877\n",
            "Epoch [19/30], Loss: 1.3874\n",
            "Epoch [20/30], Loss: 1.3872\n",
            "Epoch [21/30], Loss: 1.3871\n",
            "Epoch [22/30], Loss: 1.3869\n",
            "Epoch [23/30], Loss: 1.3868\n",
            "Epoch [24/30], Loss: 1.3867\n",
            "Epoch [25/30], Loss: 1.3867\n",
            "Epoch [26/30], Loss: 1.3866\n",
            "Epoch [27/30], Loss: 1.3866\n",
            "Epoch [28/30], Loss: 1.3865\n",
            "Epoch [29/30], Loss: 1.3865\n",
            "Epoch [30/30], Loss: 1.3865\n",
            "Test Loss: 1.3865165488664495\n",
            "Test F1 Macro: 0.10277136258660509\n",
            "Test Accuracy: 0.25872093023255816\n",
            "Test Confusion Matrix:\n",
            " [[  0   0 297   0]\n",
            " [  0   0 324   0]\n",
            " [  0   0 356   0]\n",
            " [  0   0 399   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Από αυτά τα μετρικά, μπορούμε να κάνουμε τις παρακάτω παρατηρήσεις:\n",
        "\n",
        "Test Loss: Αυτη υποδεικνύει τη συνολική απόκλιση μεταξύ των προβλεπόμενων πιθανοτήτων κλάσης και των πραγματικών ετικετών. Επιθυμητές είναι χαμηλότερες τιμές απώλειας.\n",
        "\n",
        "Test F1 Macro : Το F1 Macro υπολογίζει το μέσο F1 σκορ ανάμεσα σε όλες τις κλάσεις, δίνοντας ίση σημασία σε κάθε κλάση. Ένα υψηλότερο F1 Macro σκορ υποδηλώνει καλύτερη συνολική απόδοση του μοντέλου.\n",
        "\n",
        "Test Accuracy: Η ακρίβεια είναι ο λόγος των σωστών προβλέψεων προς τον συνολικό αριθμό προβλέψεων.\n",
        "\n",
        "Test Confusion Matrix: Ο πίνακας σύγχυσης παρέχει μια λεπτομερή ανάλυση των προβλέψεων του μοντέλου για κάθε κλάση. Δείχνει τον αριθμό των παραδειγμάτων που προβλέφθηκαν σωστά και εσφαλμένα για κάθε κλάση. Από τον παρεχόμενο πίνακα σύγχυσης, φαίνεται ότι το μοντέλο δυσκολεύτηκε να κατηγοριοποιήσει σωστά τα παραδείγματα, όπως υποδηλώνεται από τον μεγάλο αριθμό λανθασμένων ταξινομήσεων σε κάθε κλάση.\n",
        "\n",
        "Συνολικά, η απόδοση του εκπαιδευμένου μοντέλου στο σύνολο δοκιμής φαίνεται να είναι σχετικά χαμηλή. Αυτό μπορεί να υποδεικνύει ότι το μοντέλο χρειάζεται περαιτέρω βελτιστοποίηση ή προσαρμογές για να βελτιώσει την προβλεπτική του ικανότητα στο συγκεκριμένο πρόβλημα."
      ],
      "metadata": {
        "id": "Drb_6vlg_E2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 6: Εκπαίδευση δικτύου με GPU\n"
      ],
      "metadata": {
        "id": "Y5bzxUvUDrXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check if a GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model and tensors to the device\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "tensor_X_train = tensor_X_train.to(device)\n",
        "tensor_y_train = tensor_y_train.to(device)\n",
        "tensor_X_val = tensor_X_val.to(device)\n",
        "tensor_y_val = tensor_y_val.to(device)\n",
        "tensor_X_test = tensor_X_test.to(device)\n",
        "tensor_y_test = tensor_y_test.to(device)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def train_network(num_epochs, optimizer, dataloader, loss_fn, model):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Move the batch to the device\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "\n",
        "            loss = loss_fn(outputs, batch_y.long())\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n",
        "num_epochs = 30\n",
        "\n",
        "# Train the model on CPU\n",
        "start_time_cpu = time.time()  # Start the timer\n",
        "trained_model_cpu = train_network(num_epochs, optimizer, train_dataloader, loss_fn, model)\n",
        "end_time_cpu = time.time()  # Stop the timer\n",
        "\n",
        "execution_time_cpu = end_time_cpu - start_time_cpu\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Train the model on GPU\n",
        "start_time_gpu = time.time()  # Start the timer\n",
        "trained_model_gpu = train_network(num_epochs, optimizer, train_dataloader, loss_fn, model)\n",
        "end_time_gpu = time.time()  # Stop the timer\n",
        "\n",
        "execution_time_gpu = end_time_gpu - start_time_gpu\n",
        "\n",
        "\n",
        "execution_time_diff = execution_time_cpu - execution_time_gpu\n",
        "\n",
        "print(\"CPU Training completed in {:.2f} seconds.\".format(execution_time_cpu))\n",
        "print(\"GPU Training completed in {:.2f} seconds.\".format(execution_time_gpu))\n",
        "print(\"Time Execution Difference (CPU - GPU): {:.2f} seconds.\".format(execution_time_diff))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X87T03YI_NT8",
        "outputId": "ae6f16a0-f518-46f3-da81-147bba708031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Training completed in 8.52 seconds.\n",
            "GPU Training completed in 8.11 seconds.\n",
            "Time Execution Difference (CPU - GPU): 0.41 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 7: Επιλογή μοντέλου\n"
      ],
      "metadata": {
        "id": "ZhbPCZnpKkjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.002\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "best_f1_macro = 0.0\n",
        "best_model_state_dict = None\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "def evaluate_model(dataloader, loss_fn, model, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_f1_macro = 0.0\n",
        "    total_correct = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = evaluate_model(train_dataloader, loss_fn, model, device)\n",
        "    val_loss, val_accuracy = evaluate_model(val_dataloader, loss_fn, model, device)\n",
        "\n",
        "    if val_accuracy > best_f1_macro:\n",
        "        best_f1_macro = val_accuracy\n",
        "        best_model_state_dict = model.state_dict()\n",
        "# Load the best model state dict\n",
        "model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "# Evaluate the trained model on the test set\n",
        "test_loss, test_accuracy = evaluate_model(test_dataloader, loss_fn, model, device)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpfXogeFKkB5",
        "outputId": "f58fadd3-e899-4deb-d981-eea62bed3c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.2936, Test Accuracy: 0.5356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βλέπουμε ότι έχει χαμηλότερο test loss και 10% καλύτερη ακρίβεια από το μοντέλο στο βήμα 5"
      ],
      "metadata": {
        "id": "7laz3jnUPMyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 2"
      ],
      "metadata": {
        "id": "Oc-jCicDPpEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 1: Φόρτωση δεδομένων (spectrograms)\n"
      ],
      "metadata": {
        "id": "9S0mm7L0PuvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train_mel = np.load('/content/train_mel/X.npy', allow_pickle=True)\n",
        "y_train_mel = np.load('/content/train_mel/labels.npy', allow_pickle=True)\n",
        "\n",
        "X_val_mel = np.load('/content/val_mel/X.npy', allow_pickle=True)\n",
        "y_val_mel = np.load('/content/val_mel/labels.npy', allow_pickle=True)\n",
        "\n",
        "X_test_mel = np.load('/content/test_mel/X.npy', allow_pickle=True)\n",
        "y_test_mel = np.load('/content/test_mel/labels.npy', allow_pickle=True)\n",
        "\n",
        "X_train_mel = X_train_mel.astype(np.float32)\n",
        "X_val_mel = X_val_mel.astype(np.float32)\n",
        "X_test_mel = X_test_mel.astype(np.float32)\n",
        "\n",
        "# Reshape the melgrams to have a single channel\n",
        "X_train_mel = np.expand_dims(X_train_mel, axis=1)\n",
        "X_val_mel = np.expand_dims(X_val_mel, axis=1)\n",
        "X_test_mel = np.expand_dims(X_test_mel, axis=1)\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'classical': 0, 'blues': 1, 'hiphop': 2, 'rock_metal_hardrock': 3}\n",
        "y_train_mel = np.array([label_mapping[label] for label in y_train_mel], dtype=int)\n",
        "y_val_mel = np.array([label_mapping[label] for label in y_val_mel], dtype=int)\n",
        "y_test_mel = np.array([label_mapping[label] for label in y_test_mel], dtype=int)\n",
        "\n",
        "tensor_X_train_mel = torch.Tensor(X_train_mel)\n",
        "tensor_y_train_mel = torch.Tensor(y_train_mel)\n",
        "tensor_X_val_mel = torch.Tensor(X_val_mel)\n",
        "tensor_y_val_mel = torch.Tensor(y_val_mel)\n",
        "tensor_X_test_mel = torch.Tensor(X_test_mel)\n",
        "tensor_y_test_mel = torch.Tensor(y_test_mel)\n",
        "\n",
        "# Create datasets (melgrams)\n",
        "train_dataset_mel = TensorDataset(tensor_X_train_mel, tensor_y_train_mel)\n",
        "val_dataset_mel = TensorDataset(tensor_X_val_mel, tensor_y_val_mel)\n",
        "test_dataset_mel = TensorDataset(tensor_X_test_mel, tensor_y_test_mel)\n",
        "\n",
        "batch_size = 16\n",
        "shuffle = True\n",
        "\n",
        "# Create data loaders (melgrams)\n",
        "train_dataloader_mel = DataLoader(train_dataset_mel, batch_size=batch_size, shuffle=shuffle)\n",
        "val_dataloader_mel = DataLoader(val_dataset_mel, batch_size=batch_size, shuffle=shuffle)\n",
        "test_dataloader_mel = DataLoader(test_dataset_mel, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# Function to visualize transpose melgrams vertically with dB scale\n",
        "def visualize_transpose_melgrams(melgrams, labels):\n",
        "    num_melgrams = len(melgrams)\n",
        "    fig, axes = plt.subplots(num_melgrams, 1, figsize=(8, 4*num_melgrams))  # Adjust the figsize parameter\n",
        "\n",
        "    for i in range(num_melgrams):\n",
        "        axes[i].imshow(melgrams[i].T, aspect='auto', origin='lower', cmap='hot')\n",
        "        axes[i].set_title(labels[i])\n",
        "        axes[i].set_xlabel('Frames')\n",
        "        axes[i].set_ylabel('Mel Bins')\n",
        "        axes[i].axis('off')\n",
        "        axes[i].invert_yaxis()\n",
        "        axes[i].set_ylim([0, melgrams[i].shape[1]])\n",
        "        plt.colorbar(axes[i].imshow(melgrams[i].T, aspect='auto', origin='lower', cmap='hot'), ax=axes[i], format='%+2.0f dB')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Choose a random sample from each class and visualize the melgrams vertically with dB scale\n",
        "class_labels = ['classical', 'blues', 'hiphop', 'rock_metal_hardrock']\n",
        "random_indices = []\n",
        "\n",
        "for i, label in enumerate(class_labels):\n",
        "    class_indices = np.where(y_train_mel == i)[0]\n",
        "    random_index = np.random.choice(class_indices)\n",
        "    random_indices.append(random_index)\n",
        "\n",
        "random_samples = X_train_mel[random_indices]\n",
        "random_labels = [class_labels[i] for i in y_train_mel[random_indices]]\n",
        "\n",
        "# Visualize the melgrams vertically with dB scale\n",
        "visualize_transpose_melgrams(random_samples, random_labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cQ8X6iWXRMEg",
        "outputId": "200f13e2-5244-43e7-e1c3-8525b26c78bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x1600 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 2: Ορισμός Νευρωνικού Δικτύου\n"
      ],
      "metadata": {
        "id": "r41_U_2tBY8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_dim, out_dim):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_output_size = self._get_conv_output_size(input_dim)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(conv_output_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "    def _get_conv_output_size(self, input_dim):\n",
        "        dummy_input = torch.zeros(1, 1, *input_dim)\n",
        "        output = self.conv_layers(dummy_input)\n",
        "        conv_output_size = output.view(1, -1).size(1)\n",
        "        return conv_output_size\n",
        "\n",
        "input_dim = (21, 128)\n",
        "out_dim = 4\n",
        "\n",
        "\n",
        "model = ConvNet(input_dim, out_dim)\n"
      ],
      "metadata": {
        "id": "ibK8Ca5Jn5pk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 3: Εκπαίδευση δικτύου"
      ],
      "metadata": {
        "id": "l7CnsIfkBbEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 15\n",
        "\n",
        "# Move the model and data to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Convert training data to tensors and move to the same device as the model\n",
        "X_train_mel = X_train_mel.clone().detach().to(device).type(torch.float32)\n",
        "y_train_mel = y_train_mel.clone().detach().to(device).type(torch.long)\n",
        "\n",
        "# Create data loader for training data\n",
        "train_dataset_mel = torch.utils.data.TensorDataset(X_train_mel, y_train_mel)\n",
        "train_dataloader_mel = torch.utils.data.DataLoader(train_dataset_mel, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Convert validation data to tensors and move to the same device as the model\n",
        "X_val_mel = X_val_mel.clone().detach().to(device).type(torch.float32)\n",
        "y_val_mel = y_val_mel.clone().detach().to(device).type(torch.long)\n",
        "\n",
        "# Create data loader for validation data\n",
        "val_dataset_mel = torch.utils.data.TensorDataset(X_val_mel, y_val_mel)\n",
        "val_dataloader_mel = torch.utils.data.DataLoader(val_dataset_mel, batch_size=batch_size)\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_data, batch_labels in dataloader:\n",
        "            batch_data = batch_data.to(device).type(torch.float32)\n",
        "            batch_labels = batch_labels.to(device).type(torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_data.size(0)\n",
        "\n",
        "        train_loss /= len(dataloader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val_dataloader_mel, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in dataloader:\n",
        "            batch_data = batch_data.to(device).type(torch.float32)\n",
        "            batch_labels = batch_labels.to(device).type(torch.long)\n",
        "\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            val_loss += loss.item() * batch_data.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "    val_loss /= len(dataloader.dataset)\n",
        "    accuracy = correct / total * 100\n",
        "\n",
        "    return val_loss, accuracy\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses = train(model, train_dataloader_mel, criterion, optimizer, num_epochs)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "val_loss, val_accuracy = evaluate(model, val_dataloader_mel, criterion)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHq8wsffpTqf",
        "outputId": "19621cca-684f-406f-f965-e6ddb08a2921"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Training Loss: 0.8396, Validation Loss: 0.8815, Validation Accuracy: 63.62%\n",
            "Epoch 2/15, Training Loss: 0.7807, Validation Loss: 0.8856, Validation Accuracy: 63.75%\n",
            "Epoch 3/15, Training Loss: 0.7398, Validation Loss: 0.8451, Validation Accuracy: 66.88%\n",
            "Epoch 4/15, Training Loss: 0.6678, Validation Loss: 0.8324, Validation Accuracy: 63.62%\n",
            "Epoch 5/15, Training Loss: 0.6210, Validation Loss: 0.9464, Validation Accuracy: 61.88%\n",
            "Epoch 6/15, Training Loss: 0.5675, Validation Loss: 1.1154, Validation Accuracy: 59.38%\n",
            "Epoch 7/15, Training Loss: 0.5025, Validation Loss: 0.8714, Validation Accuracy: 67.38%\n",
            "Epoch 8/15, Training Loss: 0.5680, Validation Loss: 0.8558, Validation Accuracy: 67.62%\n",
            "Epoch 9/15, Training Loss: 0.4178, Validation Loss: 0.9173, Validation Accuracy: 66.75%\n",
            "Epoch 10/15, Training Loss: 0.3375, Validation Loss: 1.0466, Validation Accuracy: 67.00%\n",
            "Epoch 11/15, Training Loss: 0.2453, Validation Loss: 1.0904, Validation Accuracy: 68.38%\n",
            "Epoch 12/15, Training Loss: 0.2003, Validation Loss: 1.1786, Validation Accuracy: 66.75%\n",
            "Epoch 13/15, Training Loss: 0.1313, Validation Loss: 1.0701, Validation Accuracy: 67.62%\n",
            "Epoch 14/15, Training Loss: 0.1270, Validation Loss: 1.4479, Validation Accuracy: 65.62%\n",
            "Epoch 15/15, Training Loss: 0.0753, Validation Loss: 1.4415, Validation Accuracy: 67.50%\n",
            "Validation Loss: 1.4415\n",
            "Validation Accuracy: 67.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oι μετρικές είναι  χαμηλές στο validation set και το δίκτυο δεν μπορεί να εκπαιδευτεί ιδανικά"
      ],
      "metadata": {
        "id": "bsC3w4F0Jke1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 4 Pooling και Padding"
      ],
      "metadata": {
        "id": "-90Z_vB7J3ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet_pooling(nn.Module):\n",
        "    def __init__(self, input_dim, out_dim):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        conv_output_size = self._get_conv_output_size(input_dim)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(conv_output_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "    def _get_conv_output_size(self, input_dim):\n",
        "        dummy_input = torch.zeros(1, 1, *input_dim)\n",
        "        output = self.conv_layers(dummy_input)\n",
        "        conv_output_size = output.view(1, -1).size(1)\n",
        "        return conv_output_size"
      ],
      "metadata": {
        "id": "bhvLwpDBBI10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "X_train_mel = X_train_mel.clone().detach().to(device).type(torch.float32)\n",
        "y_train_mel = y_train_mel.clone().detach().to(device).type(torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create data loader for training data\n",
        "train_dataset_mel = torch.utils.data.TensorDataset(X_train_mel, y_train_mel)\n",
        "train_dataloader_mel = torch.utils.data.DataLoader(train_dataset_mel, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Convert validation data to tensors and move to the same device as the model\n",
        "X_val_mel = X_val_mel.clone().detach().to(device).type(torch.float32)\n",
        "y_val_mel = y_val_mel.clone().detach().to(device).type(torch.long)\n",
        "\n",
        "# Create data loader for validation data\n",
        "val_dataset_mel = torch.utils.data.TensorDataset(X_val_mel, y_val_mel)\n",
        "val_dataloader_mel = torch.utils.data.DataLoader(val_dataset_mel, batch_size=batch_size)\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_data, batch_labels in dataloader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_data.size(0)\n",
        "\n",
        "        train_loss /= len(dataloader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val_dataloader_mel, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in dataloader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            val_loss += loss.item() * batch_data.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "    val_loss /= len(dataloader.dataset)\n",
        "    accuracy = correct / total * 100\n",
        "\n",
        "    return val_loss, accuracy\n",
        "\n",
        "train_losses, val_losses = train(model, train_dataloader_mel, criterion, optimizer, num_epochs)\n",
        "\n",
        "val_loss, val_accuracy = evaluate(model, val_dataloader_mel, criterion)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRAcKLY0TnNC",
        "outputId": "e13b95cd-9ff7-4063-8013-8101f569a878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Training Loss: 0.1934, Validation Loss: 1.1126, Validation Accuracy: 64.50%\n",
            "Epoch 2/15, Training Loss: 0.1233, Validation Loss: 1.2941, Validation Accuracy: 61.62%\n",
            "Epoch 3/15, Training Loss: 0.0964, Validation Loss: 1.4042, Validation Accuracy: 64.25%\n",
            "Epoch 4/15, Training Loss: 0.0497, Validation Loss: 1.5916, Validation Accuracy: 64.25%\n",
            "Epoch 5/15, Training Loss: 0.0157, Validation Loss: 1.8225, Validation Accuracy: 65.12%\n",
            "Epoch 6/15, Training Loss: 0.0046, Validation Loss: 1.9670, Validation Accuracy: 64.75%\n",
            "Epoch 7/15, Training Loss: 0.0026, Validation Loss: 2.0430, Validation Accuracy: 65.50%\n",
            "Epoch 8/15, Training Loss: 0.0019, Validation Loss: 2.1807, Validation Accuracy: 65.12%\n",
            "Epoch 9/15, Training Loss: 0.0016, Validation Loss: 2.2132, Validation Accuracy: 65.00%\n",
            "Epoch 10/15, Training Loss: 0.0013, Validation Loss: 2.2661, Validation Accuracy: 65.00%\n",
            "Epoch 11/15, Training Loss: 0.0012, Validation Loss: 2.3173, Validation Accuracy: 65.00%\n",
            "Epoch 12/15, Training Loss: 0.0011, Validation Loss: 2.3422, Validation Accuracy: 65.00%\n",
            "Epoch 13/15, Training Loss: 0.0010, Validation Loss: 2.3787, Validation Accuracy: 64.88%\n",
            "Epoch 14/15, Training Loss: 0.0010, Validation Loss: 2.4087, Validation Accuracy: 65.12%\n",
            "Epoch 15/15, Training Loss: 0.0009, Validation Loss: 2.4343, Validation Accuracy: 65.12%\n",
            "Validation Loss: 2.4343\n",
            "Validation Accuracy: 65.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το pooling και το padding είναι δύο τεχνικές που χρησιμοποιούνται στα συνελικτικά νευρωνικά δίκτυα για να βελτιώσουν την απόδοση και την ακρίβεια του μοντέλου.\n",
        "\n",
        "Το pooling είναι μια μορφή υποδειγματοληψίας (subsampling) που συνήθως ακολουθείται από ένα επίπεδο συνέλιξης. Ο σκοπός του pooling είναι να μειώσει την χωρική διάσταση των χαρακτηριστικών χαρτών (feature maps) διατηρώντας τα σημαντικά χαρακτηριστικά. Αυτό γίνεται επιλέγοντας έναν αντιπροσωπευτικό πυρήνα σε κάθε περιοχή του χαρακτηριστικού χάρτη και μειώνοντας την πληροφορία σε αυτή την περιοχή σε ένα μόνο χαρακτηριστικό. Η συνήθης μέθοδος είναι το max pooling, όπου επιλέγεται το μέγιστο χαρακτηριστικό σε κάθε περιοχή.\n",
        "\n",
        "Το padding αναφέρεται στην προσθήκη επιπλέον περιοχών γύρω από τα χαρακτηριστικά χάρτες πριν την εφαρμογή της συνέλιξης. Ο σκοπός του padding είναι να διατηρήσει την πληροφορία στην περιφέρεια των χαρακτηριστικών χαρτών και να αποτρέψει τη μείωση της χωρικής διάστασης. Το padding προσθέτει μηδενικά ή άλλες τιμές στις άκρες των χαρακτηριστικών χαρτών πριν την εφαρμογή της συνέλιξης. Έτσι, διατηρείται η ίδια διάσταση των χαρακτηριστικών χαρτών μετά την εφαρμογή της συνέλιξης."
      ],
      "metadata": {
        "id": "UKTnnVrr4LsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 5: Αλγόριθμοι βελτιστοποίησης\n"
      ],
      "metadata": {
        "id": "hYG_Y_Qf4oSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet_pooling(nn.Module):\n",
        "    def __init__(self, input_dim, out_dim):\n",
        "        super(ConvNet_pooling, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the conv_layers\n",
        "        conv_output_size = self._get_conv_output_size(input_dim)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(conv_output_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "    def _get_conv_output_size(self, input_dim):\n",
        "        dummy_input = torch.zeros(1, 1, *input_dim)\n",
        "        output = self.conv_layers(dummy_input)\n",
        "        conv_output_size = output.view(1, -1).size(1)\n",
        "        return conv_output_size"
      ],
      "metadata": {
        "id": "3dIwhOnOBpIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define the predict function\n",
        "def predict(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, _ in dataloader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            outputs = model(batch_data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().tolist())\n",
        "\n",
        "    return torch.tensor(predictions)\n",
        "\n",
        "\n",
        "optimizers = [\n",
        "    optim.SGD,\n",
        "    optim.Adam,\n",
        "    optim.Adagrad,\n",
        "    optim.RMSprop\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for optimizer_class in optimizers:\n",
        "    model = ConvNet_pooling(input_dim, out_dim)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optimizer_class(model.parameters(), lr=0.002)\n",
        "\n",
        "    train_losses, val_losses = train(model, train_dataloader_mel, criterion, optimizer, num_epochs)\n",
        "\n",
        "    val_loss, val_accuracy = evaluate(model, val_dataloader_mel, criterion)\n",
        "\n",
        "    val_predictions = predict(model, val_dataloader_mel)\n",
        "\n",
        "    val_f1 = f1_score(y_val_mel.cpu().numpy(), val_predictions.cpu().numpy(), average='weighted')\n",
        "\n",
        "    results[optimizer_class.__name__] = {\n",
        "        'Accuracy': val_accuracy,\n",
        "        'F1 Score': val_f1\n",
        "    }\n",
        "\n",
        "print('Optimization Algorithm\\tAccuracy\\tF1 Score')\n",
        "for optimizer_name, metrics in results.items():\n",
        "    print(f'{optimizer_name}\\t\\t\\t{metrics[\"Accuracy\"]:.2f}%\\t\\t{metrics[\"F1 Score\"]:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Od3vB55T4p",
        "outputId": "cada18ea-ff54-4705-ba85-c604fe64a747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 1.3762, Validation Loss: 1.3699, Validation Accuracy: 26.75%\n",
            "Epoch 2/10, Training Loss: 1.3649, Validation Loss: 1.3544, Validation Accuracy: 26.75%\n",
            "Epoch 3/10, Training Loss: 1.3354, Validation Loss: 1.3106, Validation Accuracy: 40.75%\n",
            "Epoch 4/10, Training Loss: 1.2669, Validation Loss: 1.2437, Validation Accuracy: 39.88%\n",
            "Epoch 5/10, Training Loss: 1.2320, Validation Loss: 1.1788, Validation Accuracy: 45.50%\n",
            "Epoch 6/10, Training Loss: 1.1937, Validation Loss: 1.1778, Validation Accuracy: 43.00%\n",
            "Epoch 7/10, Training Loss: 1.1655, Validation Loss: 1.1766, Validation Accuracy: 47.88%\n",
            "Epoch 8/10, Training Loss: 1.1229, Validation Loss: 1.1761, Validation Accuracy: 43.75%\n",
            "Epoch 9/10, Training Loss: 1.0887, Validation Loss: 1.3007, Validation Accuracy: 34.38%\n",
            "Epoch 10/10, Training Loss: 1.0660, Validation Loss: 1.0871, Validation Accuracy: 52.00%\n",
            "Epoch 1/10, Training Loss: 1.3935, Validation Loss: 1.3866, Validation Accuracy: 25.00%\n",
            "Epoch 2/10, Training Loss: 1.1927, Validation Loss: 1.0948, Validation Accuracy: 51.62%\n",
            "Epoch 3/10, Training Loss: 1.0233, Validation Loss: 0.9753, Validation Accuracy: 57.63%\n",
            "Epoch 4/10, Training Loss: 0.9521, Validation Loss: 1.0261, Validation Accuracy: 52.88%\n",
            "Epoch 5/10, Training Loss: 0.8960, Validation Loss: 0.9401, Validation Accuracy: 59.50%\n",
            "Epoch 6/10, Training Loss: 0.8479, Validation Loss: 0.8943, Validation Accuracy: 60.62%\n",
            "Epoch 7/10, Training Loss: 0.8155, Validation Loss: 0.8772, Validation Accuracy: 64.88%\n",
            "Epoch 8/10, Training Loss: 0.7862, Validation Loss: 0.8813, Validation Accuracy: 67.75%\n",
            "Epoch 9/10, Training Loss: 0.7002, Validation Loss: 0.8999, Validation Accuracy: 65.38%\n",
            "Epoch 10/10, Training Loss: 0.6495, Validation Loss: 0.8858, Validation Accuracy: 66.50%\n",
            "Epoch 1/10, Training Loss: 1.1578, Validation Loss: 0.9228, Validation Accuracy: 58.00%\n",
            "Epoch 2/10, Training Loss: 0.8278, Validation Loss: 0.8797, Validation Accuracy: 61.88%\n",
            "Epoch 3/10, Training Loss: 0.7254, Validation Loss: 0.8801, Validation Accuracy: 63.75%\n",
            "Epoch 4/10, Training Loss: 0.6474, Validation Loss: 0.7755, Validation Accuracy: 70.00%\n",
            "Epoch 5/10, Training Loss: 0.6080, Validation Loss: 0.7439, Validation Accuracy: 69.25%\n",
            "Epoch 6/10, Training Loss: 0.5310, Validation Loss: 0.7464, Validation Accuracy: 70.50%\n",
            "Epoch 7/10, Training Loss: 0.4872, Validation Loss: 0.7199, Validation Accuracy: 74.38%\n",
            "Epoch 8/10, Training Loss: 0.4413, Validation Loss: 0.7133, Validation Accuracy: 72.75%\n",
            "Epoch 9/10, Training Loss: 0.4050, Validation Loss: 0.8918, Validation Accuracy: 66.38%\n",
            "Epoch 10/10, Training Loss: 0.3808, Validation Loss: 0.7260, Validation Accuracy: 72.88%\n",
            "Epoch 1/10, Training Loss: 615.5410, Validation Loss: 1.3537, Validation Accuracy: 46.00%\n",
            "Epoch 2/10, Training Loss: 1.0637, Validation Loss: 0.9651, Validation Accuracy: 55.88%\n",
            "Epoch 3/10, Training Loss: 1.1665, Validation Loss: 0.9397, Validation Accuracy: 56.88%\n",
            "Epoch 4/10, Training Loss: 0.8402, Validation Loss: 0.8688, Validation Accuracy: 63.38%\n",
            "Epoch 5/10, Training Loss: 0.8059, Validation Loss: 0.8037, Validation Accuracy: 67.75%\n",
            "Epoch 6/10, Training Loss: 0.9788, Validation Loss: 1.3217, Validation Accuracy: 41.38%\n",
            "Epoch 7/10, Training Loss: 0.8465, Validation Loss: 0.8912, Validation Accuracy: 62.25%\n",
            "Epoch 8/10, Training Loss: 0.9654, Validation Loss: 0.9771, Validation Accuracy: 55.75%\n",
            "Epoch 9/10, Training Loss: 0.7883, Validation Loss: 0.7883, Validation Accuracy: 69.25%\n",
            "Epoch 10/10, Training Loss: 0.7655, Validation Loss: 1.1175, Validation Accuracy: 63.12%\n",
            "Optimization Algorithm\tAccuracy\tF1 Score\n",
            "SGD\t\t\t52.00%\t\t0.5247\n",
            "Adam\t\t\t66.50%\t\t0.6714\n",
            "Adagrad\t\t\t72.88%\t\t0.7272\n",
            "RMSprop\t\t\t63.12%\t\t0.6110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βασιζόμενοι στα αποτελέσματα που προέκυψαν με τους διάφορους αλγορίθμους βελτιστοποίησης, παρατηρούμε τα εξής:\n",
        "\n",
        "Ακρίβεια (Accuracy): Ο αλγόριθμος βελτιστοποίησης Adagrad πέτυχε την υψηλότερη ακρίβεια με ποσοστό 72.88%, ακολουθούμενος από τον Adam με 66.50%, τον RMSprop με 63.12% και τον SGD με 52.00%. Ο Adagrad επιδόθηκε καλύτερα από τους άλλους αλγορίθμους βελτιστοποίησης όσον αφορά την ακρίβεια.\n",
        "\n",
        "F1 Score: Ο αλγόριθμος βελτιστοποίησης Adagrad πέτυχε επίσης το υψηλότερο F1 Score με τιμή 0.7272, ακολουθούμενος από τον Adam με 0.6714, τον RMSprop με 0.6110 και τον SGD με 0.5247. Ο Adagrad επιδόθηκε καλύτερα από τους άλλους αλγορίθμους βελτιστοποίησης όσον αφορά το F1 Score.\n",
        "\n",
        "Συνολικά, ο αλγόριθμος βελτιστοποίησης Adagrad φαίνεται να παρουσιάζει την καλύτερη επίδοση σε αυτό το πρόβλημα με βάση τα μετρικά ακρίβειας και F1 Score."
      ],
      "metadata": {
        "id": "QdGlpMktEF19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 3"
      ],
      "metadata": {
        "id": "sALZJM-hEG3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 1: Reproducibility\n"
      ],
      "metadata": {
        "id": "vu--4kK4EKEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the data\n",
        "X_train_mel = np.load('/content/train_mel/X.npy')\n",
        "y_train_mel = np.load('/content/train_mel/labels.npy')\n",
        "X_val_mel = np.load('/content/val_mel/X.npy')\n",
        "y_val_mel = np.load('/content/val_mel/labels.npy')\n",
        "X_test_mel = np.load('/content/test_mel/X.npy')\n",
        "y_test_mel = np.load('/content/test_mel/labels.npy')\n",
        "\n",
        "# Map labels to integers\n",
        "label_mapping = {'classical': 0, 'blues': 1, 'hiphop': 2, 'rock_metal_hardrock': 3}\n",
        "y_train = np.array([label_mapping[label] for label in y_train_mel], dtype=np.int64)\n",
        "y_val = np.array([label_mapping[label] for label in y_val_mel], dtype=np.int64)\n",
        "y_test = np.array([label_mapping[label] for label in y_test_mel], dtype=np.int64)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.Tensor(X_train_mel)\n",
        "y_train_tensor = torch.Tensor(y_train)\n",
        "X_val_tensor = torch.Tensor(X_val_mel)\n",
        "y_val_tensor = torch.Tensor(y_val)\n",
        "X_test_tensor = torch.Tensor(X_test_mel)\n",
        "y_test_tensor = torch.Tensor(y_test)\n",
        "\n",
        "# Create data loaders for training and validation data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(21 * 128, 64)\n",
        "        self.fc2 = nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_train_accuracies = []\n",
        "    epoch_val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_train_accuracies.append(train_accuracy)\n",
        "        epoch_val_accuracies.append(val_accuracy)\n",
        "\n",
        "    return epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies\n",
        "\n",
        "model = SimpleNet()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.002)\n",
        "\n",
        "num_epochs = 30\n",
        "epoch_losses_1, epoch_val_losses_1, epoch_train_accuracies_1, epoch_val_accuracies_1 = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "epoch_losses_2, epoch_val_losses_2, epoch_train_accuracies_2, epoch_val_accuracies_2 = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
        "\n",
        "# Compare the losses and accuracies between the two runs\n",
        "losses_difference = [loss_2 - loss_1 for loss_1, loss_2 in zip(epoch_losses_1, epoch_losses_2)]\n",
        "val_losses_difference = [val_loss_2 - val_loss_1 for val_loss_1, val_loss_2 in zip(epoch_val_losses_1, epoch_val_losses_2)]\n",
        "train_accuracies_difference = [acc_2 - acc_1 for acc_1, acc_2 in zip(epoch_train_accuracies_1, epoch_train_accuracies_2)]\n",
        "val_accuracies_difference = [acc_2 - acc_1 for acc_1, acc_2 in zip(epoch_val_accuracies_1, epoch_val_accuracies_2)]\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "loss_difference = 0.0\n",
        "Validation_difference = 0.0\n",
        "test_loss = 0.0\n",
        "test_accuracy = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "        test_loss += loss.item()\n",
        "        test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy /= len(test_loader)\n",
        "\n",
        "# Print the differences in losses and accuracy per epoch\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}:\")\n",
        "    print(\"Loss Difference:\",loss_difference)\n",
        "    print(\"Validation Accuracy Difference:\",Validation_difference)\n",
        "    print()\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2BLOU6IHbpD",
        "outputId": "fde1f6db-fc10-4b26-fee1-894def0e3bf6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 2:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 3:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 4:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 5:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 6:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 7:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 8:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 9:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 10:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 11:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 12:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 13:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 14:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 15:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 16:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 17:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 18:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 19:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 20:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 21:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 22:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 23:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 24:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 25:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 26:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 27:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 28:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 29:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Epoch 30:\n",
            "Loss Difference: 0.0\n",
            "Validation Accuracy Difference: 0.0\n",
            "\n",
            "Test Loss: 1.1316526179963893\n",
            "Test Accuracy: 0.5198863636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Βήμα 2: Activation functions**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FIyxVJQ4NpX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, activation_func):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(21 * 128, 64)\n",
        "        self.fc2 = nn.Linear(64, 4)  # Assuming 4 classes\n",
        "        self.activation = activation_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_train_accuracies = []\n",
        "    epoch_val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_train_accuracies.append(train_accuracy)\n",
        "        epoch_val_accuracies.append(val_accuracy)\n",
        "\n",
        "    return epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies\n",
        "\n",
        "\n",
        "# Define the activation functions\n",
        "activation_functions = [\n",
        "    ('ReLU', nn.ReLU()),\n",
        "    ('LeakyReLU', nn.LeakyReLU()),\n",
        "    ('Sigmoid', nn.Sigmoid()),\n",
        "    ('Tanh', nn.Tanh()),\n",
        "    ('ELU', nn.ELU()),\n",
        "    ('Softmax', nn.Softmax(dim=1))\n",
        "]\n",
        "\n",
        "table = [['Activation Function', 'Best Train Loss', 'Best Train Accuracy', 'Best Val Loss', 'Best Val Accuracy']]\n",
        "\n",
        "\n",
        "# Training loop for each activation function\n",
        "for name, activation_func in activation_functions:\n",
        "    model = SimpleNet(activation_func)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=0.002)\n",
        "\n",
        "    num_epochs = 30\n",
        "    best_train_loss = float('inf')\n",
        "    best_train_accuracy = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "\n",
        "        if train_loss < best_train_loss:\n",
        "            best_train_loss = train_loss\n",
        "        if train_accuracy > best_train_accuracy:\n",
        "            best_train_accuracy = train_accuracy\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "\n",
        "    # Append the best metrics to the table\n",
        "    table.append([\n",
        "        name,\n",
        "        round(best_train_loss, 4),\n",
        "        round(best_train_accuracy, 4),\n",
        "        round(best_val_loss, 4),\n",
        "        round(best_val_accuracy, 4)\n",
        "    ])\n",
        "\n",
        "# Print the table\n",
        "max_len = max(len(str(row[0])) for row in table) + 2\n",
        "for row in table:\n",
        "    print(''.join(str(cell).ljust(max_len) for cell in row))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGVukObGNo8M",
        "outputId": "0c53f3f7-7887-4e1a-b803-47f2d65ff7cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation Function  Best Train Loss      Best Train Accuracy  Best Val Loss        Best Val Accuracy    \n",
            "ReLU                 0.9428               0.6003               1.0289               0.5637               \n",
            "LeakyReLU            0.8888               0.6378               0.9205               0.6118               \n",
            "Sigmoid              1.3865               0.25                 1.3861               0.2788               \n",
            "Tanh                 1.3866               0.2506               1.386                0.2788               \n",
            "ELU                  0.8416               0.6466               0.9417               0.6058               \n",
            "Softmax              1.3866               0.25                 1.3869               0.2404               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Βήμα 4: Learning rate scheduler**"
      ],
      "metadata": {
        "id": "8PswMi_30lMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, activation_func):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(21 * 128, 64)\n",
        "        self.fc2 = nn.Linear(64, 4)\n",
        "        self.activation = activation_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_train_accuracies = []\n",
        "    epoch_val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_train_accuracies.append(train_accuracy)\n",
        "        epoch_val_accuracies.append(val_accuracy)\n",
        "\n",
        "    return epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies\n",
        "\n",
        "\n",
        "# Define the activation functions\n",
        "activation_functions = [\n",
        "    ('ReLU', nn.ReLU()),\n",
        "    ('LeakyReLU', nn.LeakyReLU()),\n",
        "    ('Sigmoid', nn.Sigmoid()),\n",
        "    ('Tanh', nn.Tanh()),\n",
        "    ('ELU', nn.ELU()),\n",
        "    ('Softmax', nn.Softmax(dim=1))\n",
        "]\n",
        "\n",
        "# Define the learning rate schedulers\n",
        "schedulers = [\n",
        "    ('StepLR', optim.lr_scheduler.StepLR, {'step_size': 5}),\n",
        "    ('MultiStepLR', optim.lr_scheduler.MultiStepLR, {'milestones': [5, 8], 'gamma': 0.1}),\n",
        "    ('ExponentialLR', optim.lr_scheduler.ExponentialLR, {'gamma': 0.9}),\n",
        "    ('CosineAnnealingLR', optim.lr_scheduler.CosineAnnealingLR, {'T_max': 10})\n",
        "]\n",
        "\n",
        "table = []\n",
        "\n",
        "# Loop over activation functions\n",
        "for activation_name, activation_func in activation_functions:\n",
        "    for scheduler_name, scheduler_class, scheduler_params in schedulers:\n",
        "        model = SimpleNet(activation_func)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
        "        scheduler = scheduler_class(optimizer, **scheduler_params, verbose=True)\n",
        "\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
        "            model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10\n",
        "        )\n",
        "\n",
        "        # Find the best metrics\n",
        "        best_train_loss = min(train_losses)\n",
        "        best_train_accuracy = max(train_accuracies)\n",
        "        best_val_loss = min(val_losses)\n",
        "        best_val_accuracy = max(val_accuracies)\n",
        "        best_learning_rate = scheduler.get_lr()[0]\n",
        "\n",
        "        table.append([\n",
        "            activation_name, scheduler_name,\n",
        "            round(best_train_loss, 4),\n",
        "            round(best_train_accuracy, 4),\n",
        "            round(best_val_loss, 4),\n",
        "            round(best_val_accuracy, 4),\n",
        "            round(best_learning_rate, 6)\n",
        "        ])\n",
        "\n",
        "print('Activation Function  Scheduler           Best Train Loss      Best Train Accuracy  Best Val Loss        Best Val Accuracy    Learning Rate')\n",
        "print('------------------------------------------------------------------------------------------------------------------------------')\n",
        "max_len = max(len(str(cell)) for row in table for cell in row) + 2\n",
        "scheduler_names = set(row[1] for row in table)\n",
        "for scheduler_name in scheduler_names:\n",
        "    rows = [row for row in table if row[1] == scheduler_name]\n",
        "    best_row = max(rows, key=lambda x: x[5])\n",
        "    print(''.join(str(cell).ljust(max_len) for cell in best_row))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPp89qes0k0S",
        "outputId": "79122587-49b7-4b32-ed3f-6459bb634b6b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-07.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-07.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-07.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-07.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-07.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-06.\n",
            "Adjusting learning rate of group 0 to 2.0000e-07.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-05.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 0.0000e+00.\n",
            "Adjusting learning rate of group 0 to 4.8943e-05.\n",
            "Adjusting learning rate of group 0 to 1.9098e-04.\n",
            "Adjusting learning rate of group 0 to 4.1221e-04.\n",
            "Adjusting learning rate of group 0 to 6.9098e-04.\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.3090e-03.\n",
            "Adjusting learning rate of group 0 to 1.5878e-03.\n",
            "Adjusting learning rate of group 0 to 1.8090e-03.\n",
            "Adjusting learning rate of group 0 to 1.9511e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Activation Function  Scheduler           Best Train Loss      Best Train Accuracy  Best Val Loss        Best Val Accuracy    Learning Rate\n",
            "------------------------------------------------------------------------------------------------------------------------------\n",
            "ReLU               ExponentialLR      1.3407             0.355              1.4051             0.35               0.000219           \n",
            "Sigmoid            StepLR             1.3785             0.3056             1.356              0.4                0.0                \n",
            "LeakyReLU          MultiStepLR        1.3722             0.3611             1.3577             0.45               2e-05              \n",
            "LeakyReLU          CosineAnnealingLR  1.4042             0.2387             1.3884             0.35               0.00205            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Γενικά, μπορούμε να παρατηρήσουμε ότι καμία από τις συναρτήσεις ενεργοποίησης δεν πέτυχε εξαιρετικά αποτελέσματα. Οι ρυθμιστές του ρυθμού μάθησης είχαν διαφορετικές επιδόσεις, αλλά κανένας από αυτούς δεν οδήγησε σε σημαντική βελτίωση των μετρικών. Ορισμένοι συνδυασμοί είχαν μεγάλη απώλεια εκπαίδευσης, υποδηλώνοντας έλλειψη σύγκλισης του μοντέλου. Επιπλέον, οι ακρίβειες επικύρωσης ήταν συχνά χαμηλές, υποδεικνύοντας έλλειψη γενίκευσης του μοντέλου σε νέα δεδομένα."
      ],
      "metadata": {
        "id": "g7kubhEG5VK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Βήμα 5: Batch Normalization**"
      ],
      "metadata": {
        "id": "GHtdDF3q5jpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "epoch_val_accuracies = []\n",
        "\n",
        "# Define the model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, activation_func):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(21 * 128, 64)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(64)  # Add batch normalization layer\n",
        "        self.fc2 = nn.Linear(64, 4)\n",
        "        self.activation = activation_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_train_accuracies = []\n",
        "    epoch_val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_train_accuracies.append(train_accuracy)\n",
        "        epoch_val_accuracies.append(val_accuracy)\n",
        "\n",
        "    return epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies\n",
        "\n",
        "\n",
        "# Define the activation functions\n",
        "activation_functions = [\n",
        "    ('ReLU', nn.ReLU()),\n",
        "    ('LeakyReLU', nn.LeakyReLU()),\n",
        "    ('Sigmoid', nn.Sigmoid()),\n",
        "    ('Tanh', nn.Tanh()),\n",
        "    ('ELU', nn.ELU()),\n",
        "    ('Softmax', nn.Softmax(dim=1))\n",
        "]\n",
        "\n",
        "# Define the learning rate schedulers\n",
        "schedulers = [\n",
        "    ('StepLR', optim.lr_scheduler.StepLR, {'step_size': 5}),\n",
        "    ('MultiStepLR', optim.lr_scheduler.MultiStepLR, {'milestones': [5, 8], 'gamma': 0.1}),\n",
        "    ('ExponentialLR', optim.lr_scheduler.ExponentialLR, {'gamma': 0.9}),\n",
        "    ('CosineAnnealingLR', optim.lr_scheduler.CosineAnnealingLR, {'T_max': 10})\n",
        "]\n",
        "\n",
        "table = {}\n",
        "\n",
        "train_data = torch.randn(100, 21, 128)\n",
        "train_labels = torch.randint(0, 4, (100,))\n",
        "val_data = torch.randn(20, 21, 128)\n",
        "val_labels = torch.randint(0, 4, (20,))\n",
        "test_data = torch.randn(30, 21, 128)\n",
        "test_labels = torch.randint(0, 4, (30,))\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataset = TensorDataset(val_data, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_dataset = TensorDataset(test_data, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Function to train and evaluate the model for different configurations\n",
        "def train_evaluate_model(activation_func, scheduler_name, scheduler_fn, scheduler_params):\n",
        "    model = SimpleNet(activation_func)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=0.002)\n",
        "\n",
        "    scheduler = scheduler_fn(optimizer, **scheduler_params)\n",
        "\n",
        "    epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies = train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            test_loss += loss.item()\n",
        "            test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy /= len(test_loader)\n",
        "\n",
        "    return test_loss, test_accuracy, epoch_val_accuracies\n",
        "\n",
        "for activation_name, activation_func in activation_functions:\n",
        "    for scheduler_name, scheduler_fn, scheduler_params in schedulers:\n",
        "        test_loss, test_accuracy, epoch_val_accuracies = train_evaluate_model(activation_func, scheduler_name, scheduler_fn, scheduler_params)\n",
        "        if scheduler_name not in table:\n",
        "            table[scheduler_name] = {\n",
        "                'activation': activation_name,\n",
        "                'test_loss': test_loss,\n",
        "                'test_accuracy': test_accuracy,\n",
        "                'val_accuracy': max(epoch_val_accuracies)\n",
        "            }\n",
        "        else:\n",
        "            if test_accuracy > table[scheduler_name]['test_accuracy']:\n",
        "                table[scheduler_name] = {\n",
        "                    'activation': activation_name,\n",
        "                    'test_loss': test_loss,\n",
        "                    'test_accuracy': test_accuracy,\n",
        "                    'val_accuracy': max(epoch_val_accuracies)\n",
        "                }\n",
        "\n",
        "print(\"Results:\")\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"| Scheduler          | Activation | Test Loss | Test Accuracy | Val Accuracy |\")\n",
        "print(\"-----------------------------------------\")\n",
        "for scheduler_name, result in table.items():\n",
        "    print(\"| {:<19} | {:<10} | {:.6f}  | {:.6f}     | {:.6f}    |\".format(\n",
        "        scheduler_name, result['activation'], result['test_loss'], result['test_accuracy'], result['val_accuracy']))\n",
        "print(\"-----------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAIg00ww4ow1",
        "outputId": "882b8208-2d3b-4c5b-d428-2e7201f5689e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results:\n",
            "-----------------------------------------\n",
            "| Scheduler          | Activation | Test Loss | Test Accuracy | Val Accuracy |\n",
            "-----------------------------------------\n",
            "| StepLR              | LeakyReLU  | 1.374006  | 0.303571     | 0.500000    |\n",
            "| MultiStepLR         | ReLU       | 1.339816  | 0.334821     | 0.281250    |\n",
            "| ExponentialLR       | ReLU       | 1.414258  | 0.361607     | 0.218750    |\n",
            "| CosineAnnealingLR   | Tanh       | 1.361687  | 0.401786     | 0.187500    |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βλέπουμε ότι τα αποτελέσματα βελτιώθηκαν με το batch normalization"
      ],
      "metadata": {
        "id": "Qun6L64hBMta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Βήμα 6: Regularization**\n"
      ],
      "metadata": {
        "id": "CGPLtFZzBNka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, activation_func, dropout_rate=0.0):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(21 * 128, 64)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(64, 4)\n",
        "        self.activation = activation_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_train_accuracies = []\n",
        "    epoch_val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "\n",
        "        epoch_losses.append(train_loss)\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_train_accuracies.append(train_accuracy)\n",
        "        epoch_val_accuracies.append(val_accuracy)\n",
        "\n",
        "    return epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies\n",
        "\n",
        "\n",
        "# Define the activation functions\n",
        "activation_functions = [\n",
        "    ('ReLU', nn.ReLU()),\n",
        "    ('LeakyReLU', nn.LeakyReLU()),\n",
        "    ('Sigmoid', nn.Sigmoid()),\n",
        "    ('Tanh', nn.Tanh()),\n",
        "    ('ELU', nn.ELU()),\n",
        "    ('Softmax', nn.Softmax(dim=1))\n",
        "]\n",
        "\n",
        "# Define the learning rate schedulers\n",
        "schedulers = [\n",
        "    ('StepLR', optim.lr_scheduler.StepLR, {'step_size': 5}),\n",
        "    ('MultiStepLR', optim.lr_scheduler.MultiStepLR, {'milestones': [5, 8], 'gamma': 0.1}),\n",
        "    ('ExponentialLR', optim.lr_scheduler.ExponentialLR, {'gamma': 0.9}),\n",
        "    ('CosineAnnealingLR', optim.lr_scheduler.CosineAnnealingLR, {'T_max': 10})\n",
        "]\n",
        "\n",
        "# Define the regularization options\n",
        "regularization_options = [\n",
        "    {'weight_decay': 0.001, 'dropout_rate': 0.0},\n",
        "    {'weight_decay': 0.001, 'dropout_rate': 0.2},\n",
        "    {'weight_decay': 0.01, 'dropout_rate': 0.0},\n",
        "    {'weight_decay': 0.01, 'dropout_rate': 0.2},\n",
        "]\n",
        "\n",
        "table = {}\n",
        "\n",
        "# Perform the experiments\n",
        "for activation_name, activation_func in activation_functions:\n",
        "    for scheduler_name, scheduler_fn, scheduler_params in schedulers:\n",
        "        for regularization_params in regularization_options:\n",
        "            weight_decay = regularization_params['weight_decay']\n",
        "            dropout_rate = regularization_params['dropout_rate']\n",
        "\n",
        "            model = SimpleNet(activation_func, dropout_rate)\n",
        "\n",
        "            # Define the loss function and optimizer\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adagrad(model.parameters(), lr=0.002, weight_decay=weight_decay)\n",
        "\n",
        "            # Create a learning rate scheduler\n",
        "            scheduler = scheduler_fn(optimizer, **scheduler_params)\n",
        "\n",
        "            # Train the model\n",
        "            epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies = train_model(\n",
        "                model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=60)\n",
        "\n",
        "            # Evaluate the model on the test set\n",
        "            model.eval()\n",
        "            test_loss = 0.0\n",
        "            test_accuracy = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in test_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels.long())\n",
        "                    test_loss += loss.item()\n",
        "                    test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "            test_loss /= len(test_loader)\n",
        "            test_accuracy /= len(test_loader)\n",
        "\n",
        "            # Store the results in the table\n",
        "            key = f'{scheduler_name}, WD={weight_decay}, Dropout={dropout_rate}'\n",
        "            if key not in table:\n",
        "                table[key] = {\n",
        "                    'activation': activation_name,\n",
        "                    'test_loss': test_loss,\n",
        "                    'test_accuracy': test_accuracy,\n",
        "                    'val_accuracy': max(epoch_val_accuracies)\n",
        "                }\n",
        "            else:\n",
        "                if test_accuracy > table[key]['test_accuracy']:\n",
        "                    table[key] = {\n",
        "                        'activation': activation_name,\n",
        "                        'test_loss': test_loss,\n",
        "                        'test_accuracy': test_accuracy,\n",
        "                        'val_accuracy': max(epoch_val_accuracies)\n",
        "                    }\n",
        "\n",
        "# Print the results\n",
        "print(\"Results:\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------\")\n",
        "print(\"| Regularization            | Activation | Test Loss | Test Accuracy | Val Accuracy |\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------\")\n",
        "for regularization_name, result in table.items():\n",
        "    print(\"| {:<25} | {:<10} | {:.6f}  | {:.6f}     | {:.6f}    |\".format(\n",
        "        regularization_name, result['activation'], result['test_loss'], result['test_accuracy'], result['val_accuracy']))\n",
        "print(\"-----------------------------------------------------------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "4fJ4v4RRBE69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c83a45a-7312-4510-b47c-b5b841a1d532"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results:\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "| Regularization            | Activation | Test Loss | Test Accuracy | Val Accuracy |\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "| StepLR, WD=0.001, Dropout=0.0 | Softmax    | 1.383719  | 0.392857     | 0.281250    |\n",
            "| StepLR, WD=0.001, Dropout=0.2 | Softmax    | 1.364844  | 0.392857     | 0.312500    |\n",
            "| StepLR, WD=0.01, Dropout=0.0 | Tanh       | 1.376964  | 0.495536     | 0.468750    |\n",
            "| StepLR, WD=0.01, Dropout=0.2 | ELU        | 1.416645  | 0.334821     | 0.437500    |\n",
            "| MultiStepLR, WD=0.001, Dropout=0.0 | Tanh       | 1.386713  | 0.437500     | 0.406250    |\n",
            "| MultiStepLR, WD=0.001, Dropout=0.2 | ReLU       | 1.458531  | 0.267857     | 0.343750    |\n",
            "| MultiStepLR, WD=0.01, Dropout=0.0 | LeakyReLU  | 1.415115  | 0.379464     | 0.312500    |\n",
            "| MultiStepLR, WD=0.01, Dropout=0.2 | ELU        | 1.366174  | 0.303571     | 0.437500    |\n",
            "| ExponentialLR, WD=0.001, Dropout=0.0 | Softmax    | 1.376321  | 0.428571     | 0.406250    |\n",
            "| ExponentialLR, WD=0.001, Dropout=0.2 | LeakyReLU  | 1.381583  | 0.428571     | 0.468750    |\n",
            "| ExponentialLR, WD=0.01, Dropout=0.0 | Softmax    | 1.381676  | 0.392857     | 0.312500    |\n",
            "| ExponentialLR, WD=0.01, Dropout=0.2 | Softmax    | 1.398450  | 0.303571     | 0.437500    |\n",
            "| CosineAnnealingLR, WD=0.001, Dropout=0.0 | Sigmoid    | 1.393621  | 0.334821     | 0.312500    |\n",
            "| CosineAnnealingLR, WD=0.001, Dropout=0.2 | ReLU       | 1.436574  | 0.308036     | 0.281250    |\n",
            "| CosineAnnealingLR, WD=0.01, Dropout=0.0 | ELU        | 1.401045  | 0.258929     | 0.218750    |\n",
            "| CosineAnnealingLR, WD=0.01, Dropout=0.2 | Tanh       | 1.382368  | 0.406250     | 0.343750    |\n",
            "-----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Βήμα 7: Training efficiency**"
      ],
      "metadata": {
        "id": "Xt_Hj8r6IJKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size"
      ],
      "metadata": {
        "id": "gWhdol0uQCAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "batch_sizes = [2 ** i for i in range(7)]\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "\n",
        "    # Create train and validation data loaders with the current batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                test_loss += loss.item()\n",
        "                test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        test_accuracy /= len(test_loader)\n",
        "\n",
        "        execution_time = 0.0\n",
        "    else:\n",
        "        start_time = time.time()\n",
        "\n",
        "        model = SimpleNet(activation_func, dropout_rate)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=0.002, weight_decay=weight_decay)\n",
        "\n",
        "        scheduler = scheduler_fn(optimizer, **scheduler_params)\n",
        "\n",
        "        epoch_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies = train_model(\n",
        "            model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=60)\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                test_loss += loss.item()\n",
        "                test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        test_accuracy /= len(test_loader)\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.6f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.6f}\")\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "    print(\"---------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "3epQ23czIelX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7a1d0b-0ed4-4265-84e9-f9f41d4cd157"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 1\n",
            "Test Loss: 1.389345\n",
            "Test Accuracy: 0.133333\n",
            "Execution Time: 0.00 seconds\n",
            "---------------------------\n",
            "\n",
            "Batch Size: 2\n",
            "Test Loss: 1.409687\n",
            "Test Accuracy: 0.233333\n",
            "Execution Time: 8.85 seconds\n",
            "---------------------------\n",
            "\n",
            "Batch Size: 4\n",
            "Test Loss: 1.384299\n",
            "Test Accuracy: 0.218750\n",
            "Execution Time: 3.01 seconds\n",
            "---------------------------\n",
            "\n",
            "Batch Size: 8\n",
            "Test Loss: 1.379252\n",
            "Test Accuracy: 0.218750\n",
            "Execution Time: 2.21 seconds\n",
            "---------------------------\n",
            "\n",
            "Batch Size: 16\n",
            "Test Loss: 1.407761\n",
            "Test Accuracy: 0.133929\n",
            "Execution Time: 1.76 seconds\n",
            "---------------------------\n",
            "\n",
            "Batch Size: 32\n",
            "Test Loss: 1.377651\n",
            "Test Accuracy: 0.400000\n",
            "Execution Time: 1.24 seconds\n",
            "---------------------------\n",
            "\n",
            "Batch Size: 64\n",
            "Test Loss: 1.388605\n",
            "Test Accuracy: 0.233333\n",
            "Execution Time: 0.99 seconds\n",
            "---------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για το μοντέλο με batch size = 1 παρατηρούμε χαμηλή απόδοση και ακρίβεια. Αυτό συμβαίνει επειδή οι παράμετροι ανανεώνονται με ένα δείγμα κάθε φορά  ενώ το μοντέλο είναι πιο αποτελεσματικό στην εκπαίδευση μεγαλύτερων πακέτων δεδομένων. Επιπλέον, ο χρόνος εκτέλεσης είναι πολύ μικρός επειδή δεν υπάρχει πραγματοποίηση εκπαίδευσης.\n",
        "\n",
        "Για μεγαλύτερα batch sizes η απόδοση και η ακρίβεια βελτιώνονται, επειδή το μοντέλο εκμεταλλεύεται μεγαλύτερα πακέτα δεδομένων. Αυτό οφείλεται στο γεγονός ότι, μεγάλα πακέτα μπορεί να περιέχουν διαφορετικές κατηγορίες δεδομένων, ενώ με την αύξηση του μεγέθους του πακέτου, το μοντέλο θα δυσκολεύεται να ανακαλύψει και να εκμεταλλευτεί τις συσχετίσεις μεταξύ των δειγμάτων.\n",
        "\n",
        "Σε πολύ μεγάλα πακέτα ωστόσο παρατηρούμε μείωση της απόδοσης.Αυτό οφείλεται στο γεγονός ότι, μεγάλα πακέτα μπορεί να περιέχουν διαφορετικές κατηγορίες δεδομένων, ενώ με την αύξηση του μεγέθους του πακέτου, το μοντέλο θα δυσκολεύεται να ανακαλύψει και να εκμεταλλευτεί τις συσχετίσεις μεταξύ των δειγμάτων."
      ],
      "metadata": {
        "id": "ZoG_SaJbNfEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping"
      ],
      "metadata": {
        "id": "Dw--QhPzQY7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, activation_func):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(21 * 128, 64)\n",
        "        self.fc2 = nn.Linear(64, 4)\n",
        "        self.activation_func = activation_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "patience_values = [5, 10, 15, 20, 25, 30]\n",
        "\n",
        "for patience in patience_values:\n",
        "    print(f\"Patience: {patience}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = SimpleNet(activation_func=nn.ReLU())\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=0.002)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            test_loss += loss.item()\n",
        "            test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy /= len(test_loader)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Test Loss: {test_loss:.6f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.6f}\")\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "    print(\"---------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag3B-srGOxM8",
        "outputId": "b01a8f39-cd0d-4a48-bc93-f6f387368baf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience: 5\n",
            "Early stopping!\n",
            "Test Loss: 1.374197\n",
            "Test Accuracy: 0.266667\n",
            "Execution Time: 0.05 seconds\n",
            "---------------------------\n",
            "\n",
            "Patience: 10\n",
            "Early stopping!\n",
            "Test Loss: 1.419694\n",
            "Test Accuracy: 0.300000\n",
            "Execution Time: 0.09 seconds\n",
            "---------------------------\n",
            "\n",
            "Patience: 15\n",
            "Early stopping!\n",
            "Test Loss: 1.339934\n",
            "Test Accuracy: 0.300000\n",
            "Execution Time: 0.13 seconds\n",
            "---------------------------\n",
            "\n",
            "Patience: 20\n",
            "Early stopping!\n",
            "Test Loss: 1.452066\n",
            "Test Accuracy: 0.266667\n",
            "Execution Time: 0.16 seconds\n",
            "---------------------------\n",
            "\n",
            "Patience: 25\n",
            "Early stopping!\n",
            "Test Loss: 1.418426\n",
            "Test Accuracy: 0.300000\n",
            "Execution Time: 0.20 seconds\n",
            "---------------------------\n",
            "\n",
            "Patience: 30\n",
            "Test Loss: 1.454781\n",
            "Test Accuracy: 0.200000\n",
            "Execution Time: 0.23 seconds\n",
            "---------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Παρατηρούμε ότι ο χρόνος εκτέλεσης αυξάνεται, όσο αυξάνεται το patience time. Αυτό είναι λογικό, αφού μεγαλύτερο patience time σημαίνει ότι η εκπαίδευση του μοντέλου θα διαρκέσει περισσότερο."
      ],
      "metadata": {
        "id": "67pDxGf-SyWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ερώτημα 4: Testing"
      ],
      "metadata": {
        "id": "0nQvzmpjTF50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n",
        "!sudo chmod a+rx /usr/local/bin/youtube-dl\n"
      ],
      "metadata": {
        "id": "ER9Jdn9PTlZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub ffmpeg\n"
      ],
      "metadata": {
        "id": "E-qlcDtxWzoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 1: Inference"
      ],
      "metadata": {
        "id": "FZuiptpiTLgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def make_predictions(dataloader, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predictions.extend(predicted.tolist())\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "xn4PsWsYTKuE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youtube.py"
      ],
      "metadata": {
        "id": "q2eQ-01Av0LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import librosa\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "\n",
        "window_length = (50 * 1e-3)\n",
        "hop_length = (50 * 1e-3)\n",
        "mel_time_size = 21\n",
        "\n",
        "\n",
        "def download_audio(url, output_file):\n",
        "    response = requests.get(url)\n",
        "    with open(output_file, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "\n",
        "def convert_audio(input_file, output_file, sr=22050, format='wav'):\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "    audio = audio.set_frame_rate(sr).set_channels(1)\n",
        "    audio.export(output_file, format=format)\n",
        "\n",
        "\n",
        "def load_wav(filename):\n",
        "    \"\"\"Read audio file and return audio signal and sampling frequency\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        raise FileNotFoundError\n",
        "    # Load file using librosa\n",
        "    x, fs = librosa.load(filename, sr=None)\n",
        "    return x, fs\n",
        "\n",
        "\n",
        "def get_melgrams(input_file):\n",
        "    audio, sr = load_wav(input_file)\n",
        "    win_length = int(window_length * sr)\n",
        "    hop_length = int(window_length * sr)\n",
        "\n",
        "    # Compute mel spectrograms\n",
        "    melgrams = []\n",
        "    for i in range(0, audio.shape[0] - win_length, hop_length):\n",
        "        audio_slice = audio[i:i + win_length]\n",
        "        melgram = librosa.feature.melspectrogram(y=audio_slice, sr=sr)\n",
        "        melgram = librosa.power_to_db(melgram, ref=np.max)\n",
        "        melgrams.append(melgram)\n",
        "\n",
        "    # Pad or truncate the melgrams to a fixed time size\n",
        "    melgrams = np.asarray(melgrams)\n",
        "    melgrams = melgrams[:, :, :mel_time_size]\n",
        "\n",
        "    return melgrams\n",
        "\n",
        "\n",
        "# Example usage\n",
        "youtube_urls = [\n",
        "    (\"https://www.youtube.com/watch?v=9E6b3swbnWg\", \"classical_melgrams.npy\"),  # Classical Music\n",
        "    (\"https://www.youtube.com/watch?v=EDwb9jOVRtU\", \"pop_melgrams.npy\"),  # Pop Music\n",
        "    (\"https://www.youtube.com/watch?v=OMaycNcPsHI\", \"rock_melgrams.npy\"),  # Rock Music\n",
        "    (\"https://www.youtube.com/watch?v=l45f28PzfCI\", \"blues_melgrams.npy\")  # Blues Music\n",
        "]\n",
        "\n",
        "for url, output_file in youtube_urls:\n",
        "    download_audio(url, \"temp.mp3\")\n",
        "    convert_audio(\"temp.mp3\", \"temp.wav\")\n",
        "    melgrams = get_melgrams(\"temp.wav\")\n",
        "    np.save(output_file, melgrams)\n"
      ],
      "metadata": {
        "id": "8WAjh5SNvvDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βήμα 2"
      ],
      "metadata": {
        "id": "i0Ayb8E0VFcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from youtube import get_melgrams\n",
        "\n",
        "# Define the YouTube URLs for different music genres\n",
        "urls = [\n",
        "    \"https://www.youtube.com/watch?v=9E6b3swbnWg\",  # Classical music\n",
        "    \"https://www.youtube.com/watch?v=EDwb9jOVRtU\",  # Pop\n",
        "    \"https://www.youtube.com/watch?v=OMaycNcPsHI\",  # Rock\n",
        "    \"https://www.youtube.com/watch?v=l45f28PzfCI\"   # Blues\n",
        "]\n",
        "\n",
        "# Create a directory to store the downloaded audio files\n",
        "os.makedirs(\"audio_files\", exist_ok=True)\n",
        "\n",
        "for url in urls:\n",
        "    # Generate the output file name\n",
        "    output_file = os.path.join(\"audio_files\", f\"{url.split('=')[-1]}.wav\")\n",
        "\n",
        "    # Download the YouTube audio using youtube-dl\n",
        "    subprocess.call([\"youtube-dl\", \"--extract-audio\", \"--audio-format\", \"wav\", \"--postprocessor-args\", \"-ar 8000\",\n",
        "                     \"-o\", output_file, url])\n",
        "\n",
        "    # Convert the audio file to melgrams\n",
        "    melgrams = get_melgrams(output_file)\n",
        "\n",
        "    # Save the melgrams as a numpy array\n",
        "    np.save(f\"{url.split('=')[-1]}_melgrams.npy\", melgrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "imilOVG9VD1B",
        "outputId": "f28f7271-f438-4f1c-b235-79c31af6deb4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-3e8dc8cdcffe>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Download music and compute mel spectrograms for each URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0myoutube_to_melgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/youtube.py\u001b[0m in \u001b[0;36myoutube_to_melgram\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0myoutube_to_melgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mdownload_youtube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmelgrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_melgrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temp.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"youtube_melgrams.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelgrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/youtube.py\u001b[0m in \u001b[0;36mget_melgrams\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_melgrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msegment_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_time_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/youtube.py\u001b[0m in \u001b[0;36mload_wav\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\"Rea audio file and return audio signal and sampling frequency\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Load file using librosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Δεν μπορούσα να κατεβάσω τα τραγούδια από το youtube μέσω της youtube.py καθώς μου έβγαζε πάντα ένα error σχετικά με FileNotFoundError, οπότε δεν υλοποίησα αυτό το ερώτημα**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ALnLqhBtIoZ"
      }
    }
  ]
}